{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cumulative_data_preparation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iK4cHRWBNeL-","outputId":"fc926b93-b409-4296-db4d-303760f8d64a"},"source":["#######################################\n","###!@0 START INIT ENVIRONMENT\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","#!wget -q https://mirrors.estointernet.in/apache/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz -P /content/drive/MyDrive # link wrong in blog\n","!tar xf /content/drive/Shareddrives/DA231-2021-Aug-Public/spark-3.0.3-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n","\n","###!@0 END INIT ENVIRONMENT"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"aH5Dhq-INkcC","outputId":"d5322fb1-81b5-42c3-9598-c8a6c5b9dd83"},"source":["#######################################\n","###!@1 START OF PYSPARK INIT\n","import findspark\n","findspark.init()\n","findspark.find()\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder\\\n","         .master(\"local\")\\\n","         .appName(\"Colab\")\\\n","         .config('spark.ui.port', '4050')\\\n","         .getOrCreate()\n","spark\n","# Spark is ready to go within Colab!\n","###!@1 END OF PYSPARK INIT"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://ab815c739753:4050\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.0.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Colab</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f8cd9cb76d0>"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"k8xXnBE8VKE3"},"source":["#Common Imports\n","from datetime import datetime\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import col\n","import pyspark.sql.functions as F\n","from pyspark import SparkContext, SparkConf\n","import sys\n","import json\n","import pandas as pd\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YnyU48H-uNoE"},"source":["# Folder ID of a folder in my Google Drive. This has to be changed\n","fid = \"1aWbFBZuxGqNouFyxGWQkRIG5vd58JPgQ\"\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive2 = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vo7w07mYNoAS"},"source":["sc = SparkContext.getOrCreate()\n","\n","drivepath = \"/content/drive/Shareddrives/ProjectSharedDrive/wazirxCSVData\"\n","driveDAILYpath = \"/content/drive/Shareddrives/ProjectSharedDrive/wazirxDAILY\"\n","\n","\n","tickers = open(drivepath+\"/tickers\" ,\"r\")\n","tfile = json.load(tickers)\n","tickers.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FlPyTOOwbvy_"},"source":["Get the list of coins from **tickers** file.\n","Choose only **USDT** Coins"]},{"cell_type":"code","metadata":{"id":"Gpo3R0o12k_A"},"source":["coins = list(tfile.keys())\n","USDTcoins = [s for s in coins if \"usdt\" in s]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G92vdI-ub7xY"},"source":["Sanity check in case the data upload is interrupted in between.\n","Get the list of files and compare with the list created. \n","Upload only the coins which are not uploaded yet"]},{"cell_type":"code","metadata":{"id":"VBtxD3l6OVJN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b536f2c-3725-4e45-9594-1f6fcce82ba3"},"source":["\n","uploaded = drive2.ListFile({'q': \"title contains 'csv' \"}).GetList()\n","\n","alreadyUploaded = []\n","for fil in uploaded:\n","  alreadyUploaded.append(fil['originalFilename'])\n","\n","coinsUploaded = [s[:-10] for s in alreadyUploaded if \"HOURLY\" in s]\n","coinsNotUploaded = [s for s in USDTcoins if s not in coinsUploaded]\n","\n","print(len(coinsNotUploaded))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"markdown","metadata":{"id":"i5u_u76ScOM2"},"source":["Creating Cumulative Data\n","\n","**Hourly to Daily**\n","\n","**Date**: Day starts at 02-11-2021  00:00:00 and ends at 23:00:00. Date must be in 02-11-2021 format\n","\n","**Open**: Use the same Hourly.'Open' value at 00:00:00\n","\n","**High**: Use the Max of Hourly.'High' Column\n","\n","**Low**: Use of Min of Hourly.'Low' Column\n","\n","**Close**: Use the same Hourly.'Close' value at 23:00:00\n","\n","**Volume**: Get the Sum of Hourly.'Volumeâ€™\n","\n","Similar logic for **Daily to Monthly** also\n"]},{"cell_type":"code","metadata":{"id":"utZ8izeZvCie"},"source":["for coin in coinsNotUploaded:\n","  coinname = coin\n","  print(\"Processing \"+ coin)\n","  coinfile = drivepath+\"/\"+coinname+\".csv\"\n","  coinDF1 = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(coinfile)\n","  hourDF = coinDF1.dropDuplicates()\n","\n","  #DAILY\n","  dayDF = hourDF.dropDuplicates().withColumn(\"Daily\", col(\"Date\")[0:10])\n","\n","  DailyCumulative = dayDF.groupBy([\"Daily\"]).agg(F.first('Open').alias('Open'), F.max('High').alias('High'), F.min('Low').alias('Low'),\\\n","                                                F.last('Close').alias('Close'), F.sum('Volume').alias('Volume'), F.count('Daily').alias('Count'))\\\n","                                                .withColumnRenamed(\"Daily\", \"Date\").orderBy([\"Date\"])\n","  DailyDF = DailyCumulative.filter(col(\"Count\") > 18).drop(\"Count\")\n","\n","  #MONTHLY\n","  monthDF = DailyCumulative.withColumn(\"Monthly\", col(\"Date\")[0:7])\n","\n","  MonthlyCumulative = monthDF.groupBy([\"Monthly\"]).agg(F.first('Open').alias('Open'), F.max('High').alias('High'), F.min('Low').alias('Low'),\\\n","                                                F.last('Close').alias('Close'), F.sum('Volume').alias('Volume'), F.count('Monthly').alias('Count'))\\\n","                                                .withColumnRenamed(\"Monthly\", \"Date\").orderBy([\"Date\"])\n","\n","  MonthlyDF = MonthlyCumulative.filter(col(\"Count\") > 20).drop(\"Count\")\n","\n","  #CSV Files in Local Storage\n","  hourDF.toPandas().to_csv(coinname+\"HOURLY.csv\")\n","  DailyDF.toPandas().to_csv(coinname+\"DAILY.csv\")\n","  MonthlyDF.toPandas().to_csv(coinname+\"MONTHLY.csv\")\n","\n","\n","  #Upload to Google Drive\n","  f = drive2.CreateFile({'title': coinname+\"HOURLY.csv\", 'parents':[{'id': fid}]})\n","  f.SetContentFile(coinname+\"HOURLY.csv\")\n","  f.Upload()\n","\n","  f = drive2.CreateFile({'title': coinname+\"DAILY.csv\", 'parents':[{'id': fid}]})\n","  f.SetContentFile(coinname+\"DAILY.csv\")\n","  f.Upload()\n","\n","  f = drive2.CreateFile({'title': coinname+\"MONTHLY.csv\", 'parents':[{'id': fid}]})\n","  f.SetContentFile(coinname+\"MONTHLY.csv\")\n","  f.Upload()\n","  print(\"Uploaded \"+coin)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1R4a-41P-BP"},"source":[""]}]}